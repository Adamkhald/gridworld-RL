# GridWorld RL ğŸ®

A comprehensive reinforcement learning playground for exploring and mastering grid world environments. This project provides an interactive framework for experimenting with various RL algorithms, visualizing agent behavior, and understanding the fundamentals of intelligent decision-making in discrete spaces.

![Grid World Agent Navigation](https://raw.githubusercontent.com/openai/gym/master/docs/_static/img/cart_pole.gif)
*Example of an RL agent learning in an environment*

> **Note**: Add your own training GIFs to showcase your agents! Place them in an `assets/` folder and reference them like: `![Agent Demo](assets/training_demo.gif)`

## ğŸ¯ About This Project

GridWorld RL is designed as an educational and experimental platform for understanding reinforcement learning concepts through grid-based environments. Built on top of the Gymnasium framework, it offers a hands-on approach to learning how agents interact with their environment, make decisions, and improve over time.

## ğŸ¤– What Are Grid Worlds?

Grid worlds are discrete environments represented as 2D grids where agents navigate from cell to cell. Each cell can contain:
- **Empty spaces** - Free movement
- **Obstacles** - Blocking positions
- **Goals** - Target destinations
- **Rewards/Penalties** - Feedback signals

<div align="center">
  <img src="https://gymnasium.farama.org/_images/minigrid-doorkey.gif" alt="MiniGrid Environment" width="300"/>
  <p><em>Agent navigating a MiniGrid environment with doors and keys</em></p>
</div>

These simple yet powerful environments are perfect for understanding core RL concepts like:
- State representation
- Action selection
- Reward shaping
- Policy learning
- Value estimation

## ğŸ§  Reinforcement Learning Fundamentals

This project implements and compares multiple RL algorithms:

### **Q-Learning**
- Model-free, off-policy algorithm
- Learns optimal action-value function
- Balances exploration vs exploitation

### **SARSA (State-Action-Reward-State-Action)**
- On-policy temporal difference learning
- Updates based on actual agent behavior
- More conservative than Q-Learning

### **DQN (Deep Q-Network)**
- Neural network-based Q-Learning
- Handles larger state spaces
- Uses experience replay and target networks

<div align="center">
  <img src="https://user-images.githubusercontent.com/12345678/example.gif" alt="Training Progress" width="400"/>
  <p><em>ğŸ“Š Add your training progress GIFs here to show learning curves!</em></p>
</div>

## ğŸ—ï¸ Built With Gymnasium

This project leverages **Gymnasium** (formerly OpenAI Gym), the standard framework for developing and comparing RL algorithms. Gymnasium provides:
- Standardized environment interface
- Consistent action and observation spaces
- Built-in rendering capabilities
- Extensive environment library compatibility

## âœ¨ Features

- **Multiple Grid World Environments**: Experiment with various layouts and challenges
- **Agent Comparison**: Run multiple algorithms side-by-side
- **Performance Tracking**: Visualize learning curves, rewards, and convergence
- **Customizable Parameters**: Tune hyperparameters like learning rate, discount factor, and exploration strategy
- **Interactive Visualizations**: Watch your agents learn in real-time
- **Comprehensive Documentation**: Auto-generated docs for all components

## ğŸ“¸ Visual Examples

To add your own training visualizations:

1. Create an `assets/` or `docs/images/` folder in your repository
2. Save your training GIFs/images there
3. Reference them in the README like this:

```markdown
![Training Demo](assets/q_learning_training.gif)
![Agent Performance](assets/agent_comparison.png)
```

**Tip**: Use tools like `matplotlib` with `animation` module or `gymnasium`'s built-in rendering to generate training GIFs!

## ğŸš€ Getting Started

```bash
# Clone the repository
git clone https://github.com/yourusername/gridworld-RL.git

# Install dependencies
pip install -r requirements.txt

# Run a sample experiment
python main.py --algorithm q-learning --episodes 1000
```

## ğŸ“Š Example Use Cases

- Learn RL fundamentals through hands-on experimentation
- Prototype new algorithm variations
- Benchmark different approaches on custom environments
- Visualize policy evolution over training
- Compare exploration strategies (Îµ-greedy, softmax, UCB)

## ğŸ‘¨â€ğŸ’» About the Author

**Adam Khald**  
2nd Year AI & Data Science Engineering Student  
ENSAM Meknes

This project was developed as part of my journey in understanding and implementing reinforcement learning algorithms. Feel free to reach out for collaborations or discussions about RL!

## ğŸ¤ Contributing

Contributions are welcome! Whether it's:
- Adding new algorithms
- Creating custom environments
- Improving visualizations
- Fixing bugs
- Enhancing documentation

## ğŸ“š Learning Resources

- [Sutton & Barto - Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html)
- [Gymnasium Documentation](https://gymnasium.farama.org/)
- [Deep RL Course by Hugging Face](https://huggingface.co/learn/deep-rl-course)

## ğŸ“ License

This project is open source and available under the MIT License.

---

*Happy Learning! ğŸ“ğŸ¤–*