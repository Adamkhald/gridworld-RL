{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d76a3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import random\n",
    "import seaborn as sns\n",
    "from gym import spaces\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from PIL import Image\n",
    "import imageio\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6389673a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded successfully!\n",
      "üìÅ Output directories created: output/gifs, output/plots, output/models\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('output/gifs', exist_ok=True)\n",
    "os.makedirs('output/plots', exist_ok=True)\n",
    "os.makedirs('output/models', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")\n",
    "print(\"üìÅ Output directories created: output/gifs, output/plots, output/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d57b150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gridworld(height=5, width=5, start=(0,0)):\n",
    "    \"\"\"\n",
    "    Create a generic GridWorld environment with customizable dimensions.\n",
    "    \n",
    "    Args:\n",
    "        height: Grid height\n",
    "        width: Grid width\n",
    "        start: Starting position tuple (row, col)\n",
    "    \n",
    "    Returns:\n",
    "        env: Dictionary containing environment configuration\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate two distinct goal positions\n",
    "    goal1 = (random.randint(0, height-1), random.randint(0, width-1))\n",
    "    while True:\n",
    "        goal2 = (random.randint(0, height-1), random.randint(0, width-1))\n",
    "        if goal2 != goal1:\n",
    "            break\n",
    "\n",
    "    # Generate obstacle that doesn't collide with start or goals\n",
    "    while True:\n",
    "        obstacle = (random.randint(0, height-1), random.randint(0, width-1))\n",
    "        if obstacle not in [goal1, goal2, start]:\n",
    "            break\n",
    "    \n",
    "    env = {\n",
    "        \"height\": height,\n",
    "        \"width\": width,\n",
    "        \"start\": np.array(start, dtype=int),\n",
    "        \"goal1\": np.array(goal1, dtype=int),\n",
    "        \"goal2\": np.array(goal2, dtype=int),\n",
    "        \"agent_pos\": np.array(start, dtype=int),\n",
    "        \"obstacle\": np.array(obstacle, dtype=int),\n",
    "        \"observation_space\": spaces.MultiDiscrete([height, width]),\n",
    "        \"action_space\": spaces.Discrete(4)\n",
    "    }\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d57ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(env, randomize=False):\n",
    "    \"\"\"\n",
    "    Reset the environment.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment dictionary\n",
    "        randomize: If True, regenerate goals and obstacle positions\n",
    "    \"\"\"\n",
    "    if randomize:\n",
    "        height = env[\"height\"]\n",
    "        width = env[\"width\"]\n",
    "        start = tuple(env[\"start\"])\n",
    "        \n",
    "        # Regenerate goals\n",
    "        goal1 = (random.randint(0, height-1), random.randint(0, width-1))\n",
    "        while True:\n",
    "            goal2 = (random.randint(0, height-1), random.randint(0, width-1))\n",
    "            if goal2 != goal1:\n",
    "                break\n",
    "        \n",
    "        # Regenerate obstacle\n",
    "        while True:\n",
    "            obstacle = (random.randint(0, height-1), random.randint(0, width-1))\n",
    "            if obstacle not in [goal1, goal2, start]:\n",
    "                break\n",
    "        \n",
    "        env[\"goal1\"] = np.array(goal1, dtype=int)\n",
    "        env[\"goal2\"] = np.array(goal2, dtype=int)\n",
    "        env[\"obstacle\"] = np.array(obstacle, dtype=int)\n",
    "    \n",
    "    env[\"agent_pos\"] = env[\"start\"].copy()\n",
    "    return env[\"agent_pos\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb2de1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(env, action):\n",
    "    \"\"\"\n",
    "    Execute action in the environment.\n",
    "    \n",
    "    Actions: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "    \"\"\"\n",
    "    r, c = env[\"agent_pos\"]\n",
    "    height = env[\"height\"]\n",
    "    width = env[\"width\"]\n",
    "\n",
    "    # Proposed new position\n",
    "    new_r, new_c = r, c\n",
    "    if action == 0:      # UP\n",
    "        new_r = max(0, r - 1)\n",
    "    elif action == 1:    # RIGHT\n",
    "        new_c = min(width - 1, c + 1)\n",
    "    elif action == 2:    # DOWN\n",
    "        new_r = min(height - 1, r + 1)\n",
    "    elif action == 3:    # LEFT\n",
    "        new_c = max(0, c - 1)\n",
    "\n",
    "    proposed_pos = np.array([new_r, new_c])\n",
    "\n",
    "    # Check obstacle\n",
    "    if np.array_equal(proposed_pos, env[\"obstacle\"]):\n",
    "        reward = -5\n",
    "        done = False\n",
    "        new_pos = env[\"agent_pos\"].copy()\n",
    "    # Check goals\n",
    "    elif np.array_equal(proposed_pos, env[\"goal1\"]) or np.array_equal(proposed_pos, env[\"goal2\"]):\n",
    "        reward = 10\n",
    "        done = True\n",
    "        new_pos = proposed_pos\n",
    "    else:\n",
    "        reward = -1\n",
    "        done = False\n",
    "        new_pos = proposed_pos\n",
    "\n",
    "    env[\"agent_pos\"] = new_pos\n",
    "    return new_pos, reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5341d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_plot(env, ax=None, title=\"GridWorld Environment\"):\n",
    "    \"\"\"Render the grid with agent, goals, and obstacle.\"\"\"\n",
    "    height = env[\"height\"]\n",
    "    width = env[\"width\"]\n",
    "    grid = np.zeros((height, width))\n",
    "\n",
    "    # Get positions\n",
    "    r, c = env[\"agent_pos\"]\n",
    "    g1r, g1c = env[\"goal1\"]\n",
    "    g2r, g2c = env[\"goal2\"]\n",
    "    orr, orc = env[\"obstacle\"]\n",
    "\n",
    "    # Assign values\n",
    "    grid[orr, orc] = 3\n",
    "    grid[g1r, g1c] = 2\n",
    "    grid[g2r, g2c] = 2\n",
    "    grid[r, c] = 1\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    ax.clear()\n",
    "    ax.imshow(grid, cmap=\"Pastel1\", origin=\"upper\")\n",
    "    ax.set_xticks(range(width))\n",
    "    ax.set_yticks(range(height))\n",
    "    ax.grid(True, which=\"both\", color=\"black\", linewidth=1)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac5063a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_train(height=5, width=5, episodes=10000, gamma=0.95, \n",
    "                     epsilon=0.3, epsilon_decay=0.995, epsilon_min=0.01, \n",
    "                     max_steps=200, save_gif=True, gif_episodes=[1, 100, 500, 1000, 5000, 10000]):\n",
    "    \"\"\"\n",
    "    Monte Carlo training with dynamic environment regeneration each episode.\n",
    "    \n",
    "    Args:\n",
    "        height: Grid height\n",
    "        width: Grid width\n",
    "        episodes: Number of training episodes\n",
    "        gamma: Discount factor\n",
    "        epsilon: Initial exploration rate\n",
    "        epsilon_decay: Decay rate for epsilon\n",
    "        epsilon_min: Minimum epsilon value\n",
    "        max_steps: Maximum steps per episode\n",
    "        save_gif: Whether to save GIF animations\n",
    "        gif_episodes: List of episode numbers to capture for GIF\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting Monte Carlo Training\")\n",
    "    print(f\"Grid Size: {height}√ó{width} | Episodes: {episodes} | Œ≥: {gamma}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Initialize environment\n",
    "    env = create_gridworld(height=height, width=width)\n",
    "    n_actions = env[\"action_space\"].n\n",
    "    \n",
    "    # Q-table and returns\n",
    "    Q = np.zeros((height, width, n_actions))\n",
    "    returns = {(s, a): [] for s in [(i, j) for i in range(height) for j in range(width)] \n",
    "               for a in range(n_actions)}\n",
    "    \n",
    "    # Tracking metrics\n",
    "    rewards_per_episode = []\n",
    "    steps_per_episode = []\n",
    "    epsilon_history = []\n",
    "    success_rate = []\n",
    "    \n",
    "    # GIF frames storage\n",
    "    gif_frames = {ep: [] for ep in gif_episodes}\n",
    "    \n",
    "    for ep in range(1, episodes + 1):\n",
    "        # Reset with NEW random environment\n",
    "        reset(env, randomize=True)\n",
    "        \n",
    "        episode = []\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        # Capture frames for GIF\n",
    "        if save_gif and ep in gif_episodes:\n",
    "            fig_temp, ax_temp = plt.subplots(figsize=(6, 6))\n",
    "        \n",
    "        while not done and steps < max_steps:\n",
    "            s = tuple(env[\"agent_pos\"])\n",
    "            \n",
    "            # Œµ-greedy policy\n",
    "            if np.random.rand() < epsilon:\n",
    "                a = np.random.randint(n_actions)\n",
    "            else:\n",
    "                a = np.argmax(Q[s[0], s[1]])\n",
    "            \n",
    "            new_state, reward, done = step(env, a)\n",
    "            episode.append((s, a, reward))\n",
    "            steps += 1\n",
    "            \n",
    "            # Save frame for GIF - FIXED VERSION\n",
    "            if save_gif and ep in gif_episodes:\n",
    "                render_plot(env, ax=ax_temp, title=f\"Episode {ep} - Step {steps}\")\n",
    "                fig_temp.canvas.draw()\n",
    "                \n",
    "                # Use the modern method to get canvas buffer\n",
    "                buf = fig_temp.canvas.buffer_rgba()\n",
    "                frame = np.asarray(buf)\n",
    "                \n",
    "                # Convert RGBA to RGB\n",
    "                frame = frame[:, :, :3]\n",
    "                gif_frames[ep].append(frame)\n",
    "        \n",
    "        if save_gif and ep in gif_episodes:\n",
    "            plt.close(fig_temp)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        epsilon_history.append(epsilon)\n",
    "        \n",
    "        # Compute returns (first-visit MC)\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for (s, a, r) in reversed(episode):\n",
    "            G = gamma * G + r\n",
    "            if (s, a) not in visited:\n",
    "                returns[(s, a)].append(G)\n",
    "                Q[s[0], s[1], a] = np.mean(returns[(s, a)])\n",
    "                visited.add((s, a))\n",
    "        \n",
    "        # Track metrics\n",
    "        total_reward = sum([r for (_, _, r) in episode])\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        steps_per_episode.append(steps)\n",
    "        \n",
    "        # Success tracking\n",
    "        success = 1 if done and total_reward > 0 else 0\n",
    "        if len(success_rate) == 0:\n",
    "            success_rate.append(success)\n",
    "        else:\n",
    "            success_rate.append(0.99 * success_rate[-1] + 0.01 * success)\n",
    "        \n",
    "        # Progress logging\n",
    "        if ep % 500 == 0:\n",
    "            recent_avg = np.mean(rewards_per_episode[-100:])\n",
    "            recent_success = success_rate[-1] * 100\n",
    "            avg_steps = np.mean(steps_per_episode[-100:])\n",
    "            print(f\"Ep {ep:5d} | Avg Reward: {recent_avg:6.2f} | \" +\n",
    "                  f\"Success: {recent_success:5.1f}% | \" +\n",
    "                  f\"Avg Steps: {avg_steps:5.1f} | Œµ: {epsilon:.4f}\")\n",
    "    \n",
    "    # Save GIFs\n",
    "    if save_gif:\n",
    "        print(\"\\nüé¨ Generating GIF animations...\")\n",
    "        for ep in gif_episodes:\n",
    "            if gif_frames[ep]:\n",
    "                imageio.mimsave(f'output/gifs/monte_carlo_episode_{ep}.gif', \n",
    "                              gif_frames[ep], fps=5)\n",
    "                print(f\"‚úÖ Saved: monte_carlo_episode_{ep}.gif\")\n",
    "    \n",
    "    # Save plots\n",
    "    print(\"\\nüìä Saving performance plots...\")\n",
    "    results = {\n",
    "        'rewards': rewards_per_episode,\n",
    "        'steps': steps_per_episode,\n",
    "        'success_rate': success_rate,\n",
    "        'epsilon': epsilon_history\n",
    "    }\n",
    "    save_training_plots(results, algorithm='MonteCarlo', grid_size=f\"{height}x{width}\")\n",
    "    \n",
    "    # Save Q-table\n",
    "    np.save('output/models/monte_carlo_Q_table.npy', Q)\n",
    "    print(\"‚úÖ Saved: monte_carlo_Q_table.npy\")\n",
    "    \n",
    "    # Final stats\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéâ Monte Carlo Training Completed!\")\n",
    "    print(\"=\" * 70)\n",
    "    final_avg = np.mean(rewards_per_episode[-100:])\n",
    "    final_success = success_rate[-1] * 100\n",
    "    final_steps = np.mean(steps_per_episode[-100:])\n",
    "    print(f\"Final Avg Reward (last 100): {final_avg:.2f}\")\n",
    "    print(f\"Final Success Rate: {final_success:.1f}%\")\n",
    "    print(f\"Final Avg Steps: {final_steps:.1f}\")\n",
    "    print(f\"Final Epsilon: {epsilon:.4f}\")\n",
    "    \n",
    "    return Q, results, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b2f7450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_train(height=5, width=5, episodes=10000, gamma=0.95, \n",
    "                    alpha=0.1, epsilon=0.3, epsilon_decay=0.995, \n",
    "                    epsilon_min=0.01, max_steps=200, save_gif=True, \n",
    "                    gif_episodes=[1, 100, 500, 1000, 5000, 10000]):\n",
    "    \"\"\"\n",
    "    Q-Learning training with dynamic environment regeneration each episode.\n",
    "    \n",
    "    Args:\n",
    "        height: Grid height\n",
    "        width: Grid width\n",
    "        episodes: Number of training episodes\n",
    "        gamma: Discount factor\n",
    "        alpha: Learning rate\n",
    "        epsilon: Initial exploration rate\n",
    "        epsilon_decay: Decay rate for epsilon\n",
    "        epsilon_min: Minimum epsilon value\n",
    "        max_steps: Maximum steps per episode\n",
    "        save_gif: Whether to save GIF animations\n",
    "        gif_episodes: List of episode numbers to capture for GIF\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting Q-Learning Training\")\n",
    "    print(f\"Grid Size: {height}√ó{width} | Episodes: {episodes} | Œ≥: {gamma} | Œ±: {alpha}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Initialize environment\n",
    "    env = create_gridworld(height=height, width=width)\n",
    "    n_actions = env[\"action_space\"].n\n",
    "    \n",
    "    # Q-table\n",
    "    Q = np.zeros((height, width, n_actions))\n",
    "    \n",
    "    # Tracking metrics\n",
    "    rewards_per_episode = []\n",
    "    steps_per_episode = []\n",
    "    epsilon_history = []\n",
    "    success_rate = []\n",
    "    \n",
    "    # GIF frames storage\n",
    "    gif_frames = {ep: [] for ep in gif_episodes}\n",
    "    \n",
    "    for ep in range(1, episodes + 1):\n",
    "        # Reset with NEW random environment\n",
    "        reset(env, randomize=True)\n",
    "        \n",
    "        done = False\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Capture frames for GIF\n",
    "        if save_gif and ep in gif_episodes:\n",
    "            fig_temp, ax_temp = plt.subplots(figsize=(6, 6))\n",
    "        \n",
    "        while not done and steps < max_steps:\n",
    "            s = tuple(env[\"agent_pos\"])\n",
    "            \n",
    "            # Œµ-greedy action selection\n",
    "            if np.random.rand() < epsilon:\n",
    "                a = np.random.randint(n_actions)\n",
    "            else:\n",
    "                a = np.argmax(Q[s[0], s[1]])\n",
    "            \n",
    "            # Take action\n",
    "            new_state, reward, done = step(env, a)\n",
    "            s_next = tuple(env[\"agent_pos\"])\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # Q-Learning update\n",
    "            best_next_action = np.argmax(Q[s_next[0], s_next[1]])\n",
    "            td_target = reward + gamma * Q[s_next[0], s_next[1], best_next_action]\n",
    "            td_error = td_target - Q[s[0], s[1], a]\n",
    "            Q[s[0], s[1], a] += alpha * td_error\n",
    "            \n",
    "            # Save frame for GIF - FIXED VERSION\n",
    "            if save_gif and ep in gif_episodes:\n",
    "                render_plot(env, ax=ax_temp, title=f\"Episode {ep} - Step {steps}\")\n",
    "                fig_temp.canvas.draw()\n",
    "                \n",
    "                # Use the modern method to get canvas buffer\n",
    "                buf = fig_temp.canvas.buffer_rgba()\n",
    "                frame = np.asarray(buf)\n",
    "                \n",
    "                # Convert RGBA to RGB\n",
    "                frame = frame[:, :, :3]\n",
    "                gif_frames[ep].append(frame)\n",
    "        \n",
    "        if save_gif and ep in gif_episodes:\n",
    "            plt.close(fig_temp)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        epsilon_history.append(epsilon)\n",
    "        \n",
    "        # Track metrics\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        steps_per_episode.append(steps)\n",
    "        \n",
    "        # Success tracking\n",
    "        success = 1 if done and total_reward > 0 else 0\n",
    "        if len(success_rate) == 0:\n",
    "            success_rate.append(success)\n",
    "        else:\n",
    "            success_rate.append(0.99 * success_rate[-1] + 0.01 * success)\n",
    "        \n",
    "        # Progress logging\n",
    "        if ep % 500 == 0:\n",
    "            recent_avg = np.mean(rewards_per_episode[-100:])\n",
    "            recent_success = success_rate[-1] * 100\n",
    "            avg_steps = np.mean(steps_per_episode[-100:])\n",
    "            print(f\"Ep {ep:5d} | Avg Reward: {recent_avg:6.2f} | \" +\n",
    "                  f\"Success: {recent_success:5.1f}% | \" +\n",
    "                  f\"Avg Steps: {avg_steps:5.1f} | Œµ: {epsilon:.4f}\")\n",
    "    \n",
    "    # Save GIFs\n",
    "    if save_gif:\n",
    "        print(\"\\nüé¨ Generating GIF animations...\")\n",
    "        for ep in gif_episodes:\n",
    "            if gif_frames[ep]:\n",
    "                imageio.mimsave(f'output/gifs/q_learning_episode_{ep}.gif', \n",
    "                              gif_frames[ep], fps=5)\n",
    "                print(f\"‚úÖ Saved: q_learning_episode_{ep}.gif\")\n",
    "    \n",
    "    # Save plots\n",
    "    print(\"\\nüìä Saving performance plots...\")\n",
    "    results = {\n",
    "        'rewards': rewards_per_episode,\n",
    "        'steps': steps_per_episode,\n",
    "        'success_rate': success_rate,\n",
    "        'epsilon': epsilon_history\n",
    "    }\n",
    "    save_training_plots(results, algorithm='QLearning', grid_size=f\"{height}x{width}\")\n",
    "    \n",
    "    # Save Q-table\n",
    "    np.save('output/models/q_learning_Q_table.npy', Q)\n",
    "    print(\"‚úÖ Saved: q_learning_Q_table.npy\")\n",
    "    \n",
    "    # Final stats\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéâ Q-Learning Training Completed!\")\n",
    "    print(\"=\" * 70)\n",
    "    final_avg = np.mean(rewards_per_episode[-100:])\n",
    "    final_success = success_rate[-1] * 100\n",
    "    final_steps = np.mean(steps_per_episode[-100:])\n",
    "    print(f\"Final Avg Reward (last 100): {final_avg:.2f}\")\n",
    "    print(f\"Final Success Rate: {final_success:.1f}%\")\n",
    "    print(f\"Final Avg Steps: {final_steps:.1f}\")\n",
    "    print(f\"Final Epsilon: {epsilon:.4f}\")\n",
    "    \n",
    "    return Q, results, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49b1e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_plots(results, algorithm='MonteCarlo', grid_size='5x5', window=100):\n",
    "    \"\"\"Save comprehensive training plots as PNG files.\"\"\"\n",
    "    \n",
    "    rewards = results['rewards']\n",
    "    steps = results['steps']\n",
    "    success = results['success_rate']\n",
    "    epsilon = results['epsilon']\n",
    "    episodes = list(range(1, len(rewards) + 1))\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'{algorithm} Training Results - Grid {grid_size}', \n",
    "                 fontsize=18, fontweight='bold', y=0.995)\n",
    "    \n",
    "    # 1. Rewards with moving average\n",
    "    axes[0, 0].plot(episodes, rewards, alpha=0.3, color='blue', linewidth=0.8, label='Raw Reward')\n",
    "    if len(rewards) >= window:\n",
    "        moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "        axes[0, 0].plot(range(window, len(rewards) + 1), moving_avg, \n",
    "                       color='red', linewidth=2.5, label=f'{window}-Episode MA')\n",
    "    axes[0, 0].set_xlabel('Episode', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Total Reward', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_title('Training Rewards Over Time', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend(loc='lower right', fontsize=10)\n",
    "    axes[0, 0].grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # 2. Success rate\n",
    "    axes[0, 1].plot(episodes, np.array(success) * 100, color='green', linewidth=2.5)\n",
    "    axes[0, 1].fill_between(episodes, 0, np.array(success) * 100, alpha=0.3, color='green')\n",
    "    axes[0, 1].set_xlabel('Episode', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Success Rate (%)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_title('Success Rate Evolution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_ylim(0, 105)\n",
    "    axes[0, 1].grid(True, alpha=0.3, linestyle='--')\n",
    "    axes[0, 1].axhline(y=80, color='red', linestyle='--', linewidth=1.5, label='80% Target')\n",
    "    axes[0, 1].legend(loc='lower right', fontsize=10)\n",
    "    \n",
    "    # 3. Steps per episode\n",
    "    axes[1, 0].plot(episodes, steps, color='purple', alpha=0.4, linewidth=0.8, label='Raw Steps')\n",
    "    if len(steps) >= window:\n",
    "        steps_ma = np.convolve(steps, np.ones(window)/window, mode='valid')\n",
    "        axes[1, 0].plot(range(window, len(steps) + 1), steps_ma, \n",
    "                       color='darkviolet', linewidth=2.5, label=f'{window}-Episode MA')\n",
    "    axes[1, 0].set_xlabel('Episode', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Steps to Completion', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_title('Steps per Episode', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend(loc='upper right', fontsize=10)\n",
    "    axes[1, 0].grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # 4. Epsilon decay\n",
    "    axes[1, 1].plot(episodes, epsilon, color='orange', linewidth=2.5)\n",
    "    axes[1, 1].fill_between(episodes, 0, epsilon, alpha=0.3, color='orange')\n",
    "    axes[1, 1].set_xlabel('Episode', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Epsilon (Œµ)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_title('Exploration Rate Decay', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    filename = f'output/plots/{algorithm}_training_results_{grid_size}.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved: {filename}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Additional plot: Convergence analysis\n",
    "    fig2, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    if len(rewards) >= window:\n",
    "        moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "        ax.plot(range(window, len(rewards) + 1), moving_avg, \n",
    "               color='blue', linewidth=3, label='Reward MA')\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(episodes, np.array(success) * 100, color='green', \n",
    "            linewidth=3, label='Success Rate', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Episode', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Average Reward', fontsize=13, fontweight='bold', color='blue')\n",
    "    ax2.set_ylabel('Success Rate (%)', fontsize=13, fontweight='bold', color='green')\n",
    "    ax.tick_params(axis='y', labelcolor='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "    \n",
    "    ax.set_title(f'{algorithm} Convergence Analysis - Grid {grid_size}', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.legend(loc='upper left', fontsize=11)\n",
    "    ax2.legend(loc='upper right', fontsize=11)\n",
    "    \n",
    "    filename2 = f'output/plots/{algorithm}_convergence_analysis_{grid_size}.png'\n",
    "    plt.savefig(filename2, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved: {filename2}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67ac41c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(env, Q, algorithm='MonteCarlo'):\n",
    "    \"\"\"Visualize and save the learned policy.\"\"\"\n",
    "    height = env[\"height\"]\n",
    "    width = env[\"width\"]\n",
    "    arrows = {0: '‚Üë', 1: '‚Üí', 2: '‚Üì', 3: '‚Üê'}\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(max(10, width*2), max(8, height*2)))\n",
    "    \n",
    "    # Create policy grid\n",
    "    policy_grid = np.zeros((height, width))\n",
    "    \n",
    "    for r in range(height):\n",
    "        for c in range(width):\n",
    "            best_action = np.argmax(Q[r, c])\n",
    "            q_max = Q[r, c, best_action]\n",
    "            if q_max != 0:\n",
    "                policy_grid[r, c] = best_action + 1\n",
    "    \n",
    "    # Plot\n",
    "    cmap = plt.cm.get_cmap('Set3', 5)\n",
    "    im = ax.imshow(policy_grid, cmap=cmap, origin='upper', alpha=0.6)\n",
    "    \n",
    "    # Add arrows\n",
    "    for r in range(height):\n",
    "        for c in range(width):\n",
    "            best_action = np.argmax(Q[r, c])\n",
    "            q_max = Q[r, c, best_action]\n",
    "            if q_max != 0:\n",
    "                ax.text(c, r, arrows[best_action], ha='center', va='center', \n",
    "                       fontsize=20, fontweight='bold', color='black')\n",
    "            else:\n",
    "                ax.text(c, r, '?', ha='center', va='center', \n",
    "                       fontsize=16, color='gray', alpha=0.5)\n",
    "    \n",
    "    ax.set_xticks(range(width))\n",
    "    ax.set_yticks(range(height))\n",
    "    ax.set_title(f'{algorithm} Learned Policy - Grid {height}√ó{width}', \n",
    "                fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.grid(True, which='both', color='black', linewidth=1.5)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_labels = ['0: Unexplored', '1: UP ‚Üë', '2: RIGHT ‚Üí', '3: DOWN ‚Üì', '4: LEFT ‚Üê']\n",
    "    handles = [plt.Rectangle((0,0),1,1, fc=cmap(i)) for i in range(5)]\n",
    "    ax.legend(handles, legend_labels, loc='center left', bbox_to_anchor=(1, 0.5), fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'output/plots/{algorithm}_learned_policy_{height}x{width}.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved: {filename}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82e9a291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Monte Carlo Training\n",
      "Grid Size: 10√ó10 | Episodes: 10000 | Œ≥: 0.95\n",
      "----------------------------------------------------------------------\n",
      "Ep   500 | Avg Reward: -188.72 | Success:   1.5% | Avg Steps: 185.3 | Œµ: 0.0245\n",
      "Ep  1000 | Avg Reward: -198.77 | Success:   0.5% | Avg Steps: 198.6 | Œµ: 0.0100\n",
      "Ep  1500 | Avg Reward: -189.63 | Success:   3.9% | Avg Steps: 190.1 | Œµ: 0.0100\n",
      "Ep  2000 | Avg Reward: -195.96 | Success:   1.9% | Avg Steps: 196.0 | Œµ: 0.0100\n",
      "Ep  2500 | Avg Reward: -187.11 | Success:   2.6% | Avg Steps: 187.9 | Œµ: 0.0100\n",
      "Ep  3000 | Avg Reward: -193.92 | Success:   1.2% | Avg Steps: 194.4 | Œµ: 0.0100\n",
      "Ep  3500 | Avg Reward: -197.58 | Success:   1.6% | Avg Steps: 197.8 | Œµ: 0.0100\n",
      "Ep  4000 | Avg Reward: -188.94 | Success:   4.2% | Avg Steps: 189.4 | Œµ: 0.0100\n",
      "Ep  4500 | Avg Reward: -190.94 | Success:   2.1% | Avg Steps: 191.5 | Œµ: 0.0100\n",
      "Ep  5000 | Avg Reward: -191.60 | Success:   2.6% | Avg Steps: 192.2 | Œµ: 0.0100\n",
      "Ep  5500 | Avg Reward: -186.09 | Success:   2.8% | Avg Steps: 187.2 | Œµ: 0.0100\n",
      "Ep  6000 | Avg Reward: -185.43 | Success:   4.3% | Avg Steps: 186.5 | Œµ: 0.0100\n",
      "Ep  6500 | Avg Reward: -192.45 | Success:   1.9% | Avg Steps: 189.7 | Œµ: 0.0100\n",
      "Ep  7000 | Avg Reward: -197.52 | Success:   0.4% | Avg Steps: 197.7 | Œµ: 0.0100\n",
      "Ep  7500 | Avg Reward: -195.30 | Success:   1.5% | Avg Steps: 195.8 | Œµ: 0.0100\n",
      "Ep  8000 | Avg Reward: -192.00 | Success:   2.1% | Avg Steps: 190.2 | Œµ: 0.0100\n",
      "Ep  8500 | Avg Reward: -196.99 | Success:   1.6% | Avg Steps: 195.6 | Œµ: 0.0100\n",
      "Ep  9000 | Avg Reward: -187.18 | Success:   2.3% | Avg Steps: 188.2 | Œµ: 0.0100\n",
      "Ep  9500 | Avg Reward: -197.65 | Success:   1.6% | Avg Steps: 197.9 | Œµ: 0.0100\n",
      "Ep 10000 | Avg Reward: -193.59 | Success:   0.4% | Avg Steps: 192.4 | Œµ: 0.0100\n",
      "\n",
      "üé¨ Generating GIF animations...\n",
      "‚úÖ Saved: monte_carlo_episode_1.gif\n",
      "‚úÖ Saved: monte_carlo_episode_100.gif\n",
      "‚úÖ Saved: monte_carlo_episode_500.gif\n",
      "‚úÖ Saved: monte_carlo_episode_1000.gif\n",
      "‚úÖ Saved: monte_carlo_episode_5000.gif\n",
      "‚úÖ Saved: monte_carlo_episode_10000.gif\n",
      "\n",
      "üìä Saving performance plots...\n",
      "‚úÖ Saved: output/plots/MonteCarlo_training_results_10x10.png\n",
      "‚úÖ Saved: output/plots/MonteCarlo_convergence_analysis_10x10.png\n",
      "‚úÖ Saved: monte_carlo_Q_table.npy\n",
      "\n",
      "======================================================================\n",
      "üéâ Monte Carlo Training Completed!\n",
      "======================================================================\n",
      "Final Avg Reward (last 100): -193.59\n",
      "Final Success Rate: 0.4%\n",
      "Final Avg Steps: 192.4\n",
      "Final Epsilon: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABOUY\\AppData\\Local\\Temp\\ipykernel_11792\\2176497653.py:20: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = plt.cm.get_cmap('Set3', 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: output/plots/MonteCarlo_learned_policy_10x10.png\n"
     ]
    }
   ],
   "source": [
    "GRID_HEIGHT = 10\n",
    "GRID_WIDTH = 10\n",
    "\n",
    "# Train Monte Carlo\n",
    "Q_mc, results_mc, env_mc = monte_carlo_train(\n",
    "    height=GRID_HEIGHT,\n",
    "    width=GRID_WIDTH,\n",
    "    episodes=10000,\n",
    "    gamma=0.95,\n",
    "    epsilon=0.3,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.01,\n",
    "    max_steps=200,\n",
    "    save_gif=True,\n",
    "    gif_episodes=[1, 100, 500, 1000, 5000, 10000]\n",
    ")\n",
    "\n",
    "# Visualize policy\n",
    "visualize_policy(env_mc, Q_mc, algorithm='MonteCarlo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce40f30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Q-Learning Training\n",
      "Grid Size: 10√ó10 | Episodes: 10000 | Œ≥: 0.95 | Œ±: 0.1\n",
      "----------------------------------------------------------------------\n",
      "Ep   500 | Avg Reward: -78.26 | Success:  12.7% | Avg Steps:  85.0 | Œµ: 0.0245\n",
      "Ep  1000 | Avg Reward: -76.72 | Success:   9.8% | Avg Steps:  84.4 | Œµ: 0.0100\n",
      "Ep  1500 | Avg Reward: -81.58 | Success:  10.4% | Avg Steps:  89.3 | Œµ: 0.0100\n",
      "Ep  2000 | Avg Reward: -100.96 | Success:  11.5% | Avg Steps: 106.7 | Œµ: 0.0100\n",
      "Ep  2500 | Avg Reward: -107.55 | Success:  13.4% | Avg Steps: 113.1 | Œµ: 0.0100\n",
      "Ep  3000 | Avg Reward: -108.16 | Success:  10.7% | Avg Steps: 113.7 | Œµ: 0.0100\n",
      "Ep  3500 | Avg Reward: -102.57 | Success:  19.8% | Avg Steps: 107.4 | Œµ: 0.0100\n",
      "Ep  4000 | Avg Reward: -108.95 | Success:  21.3% | Avg Steps: 114.1 | Œµ: 0.0100\n",
      "Ep  4500 | Avg Reward: -156.08 | Success:  14.2% | Avg Steps: 157.8 | Œµ: 0.0100\n",
      "Ep  5000 | Avg Reward: -176.86 | Success:   8.7% | Avg Steps: 177.7 | Œµ: 0.0100\n",
      "Ep  5500 | Avg Reward: -147.84 | Success:  12.6% | Avg Steps: 150.4 | Œµ: 0.0100\n",
      "Ep  6000 | Avg Reward: -160.99 | Success:  11.2% | Avg Steps: 162.7 | Œµ: 0.0100\n",
      "Ep  6500 | Avg Reward: -148.62 | Success:  13.1% | Avg Steps: 151.0 | Œµ: 0.0100\n",
      "Ep  7000 | Avg Reward: -164.31 | Success:   7.7% | Avg Steps: 166.2 | Œµ: 0.0100\n",
      "Ep  7500 | Avg Reward: -186.35 | Success:   5.0% | Avg Steps: 186.9 | Œµ: 0.0100\n",
      "Ep  8000 | Avg Reward: -182.62 | Success:   7.1% | Avg Steps: 183.4 | Œµ: 0.0100\n",
      "Ep  8500 | Avg Reward: -165.67 | Success:   9.9% | Avg Steps: 167.5 | Œµ: 0.0100\n",
      "Ep  9000 | Avg Reward: -164.83 | Success:   9.9% | Avg Steps: 166.5 | Œµ: 0.0100\n",
      "Ep  9500 | Avg Reward: -195.11 | Success:   2.9% | Avg Steps: 195.3 | Œµ: 0.0100\n",
      "Ep 10000 | Avg Reward: -180.02 | Success:   9.0% | Avg Steps: 181.0 | Œµ: 0.0100\n",
      "\n",
      "üé¨ Generating GIF animations...\n",
      "‚úÖ Saved: q_learning_episode_1.gif\n",
      "‚úÖ Saved: q_learning_episode_100.gif\n",
      "‚úÖ Saved: q_learning_episode_500.gif\n",
      "‚úÖ Saved: q_learning_episode_1000.gif\n",
      "‚úÖ Saved: q_learning_episode_5000.gif\n",
      "‚úÖ Saved: q_learning_episode_10000.gif\n",
      "\n",
      "üìä Saving performance plots...\n",
      "‚úÖ Saved: output/plots/QLearning_training_results_10x10.png\n",
      "‚úÖ Saved: output/plots/QLearning_convergence_analysis_10x10.png\n",
      "‚úÖ Saved: q_learning_Q_table.npy\n",
      "\n",
      "======================================================================\n",
      "üéâ Q-Learning Training Completed!\n",
      "======================================================================\n",
      "Final Avg Reward (last 100): -180.02\n",
      "Final Success Rate: 9.0%\n",
      "Final Avg Steps: 181.0\n",
      "Final Epsilon: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABOUY\\AppData\\Local\\Temp\\ipykernel_11792\\2176497653.py:20: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = plt.cm.get_cmap('Set3', 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: output/plots/QLearning_learned_policy_10x10.png\n"
     ]
    }
   ],
   "source": [
    "Q_ql, results_ql, env_ql = q_learning_train(\n",
    "    height=GRID_HEIGHT,\n",
    "    width=GRID_WIDTH,\n",
    "    episodes=10000,\n",
    "    gamma=0.95,\n",
    "    alpha=0.1,\n",
    "    epsilon=0.3,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.01,\n",
    "    max_steps=200,\n",
    "    save_gif=True,\n",
    "    gif_episodes=[1, 100, 500, 1000, 5000, 10000]\n",
    ")\n",
    "\n",
    "# Visualize policy\n",
    "visualize_policy(env_ql, Q_ql, algorithm='QLearning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfbef269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: output/plots/algorithm_comparison_10x10.png\n",
      "\n",
      "======================================================================\n",
      "üìä ALGORITHM COMPARISON SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Metric                             Monte Carlo      Q-Learning     Winner\n",
      "----------------------------------------------------------------------\n",
      "Avg Reward (last 100)                  -193.59         -180.02         QL\n",
      "Success Rate (%)                           0.4             9.0         QL\n",
      "Avg Steps (last 100)                     192.4           181.0         QL\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def compare_algorithms(results_mc, results_ql, grid_size='5x5'):\n",
    "    \"\"\"Compare Monte Carlo and Q-Learning performance.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'Algorithm Comparison: Monte Carlo vs Q-Learning - Grid {grid_size}', \n",
    "                 fontsize=18, fontweight='bold', y=0.995)\n",
    "    \n",
    "    window = 100\n",
    "    episodes = list(range(1, len(results_mc['rewards']) + 1))\n",
    "    \n",
    "    # 1. Rewards comparison\n",
    "    if len(results_mc['rewards']) >= window:\n",
    "        mc_ma = np.convolve(results_mc['rewards'], np.ones(window)/window, mode='valid')\n",
    "        ql_ma = np.convolve(results_ql['rewards'], np.ones(window)/window, mode='valid')\n",
    "        ma_episodes = list(range(window, len(results_mc['rewards']) + 1))\n",
    "        \n",
    "        axes[0, 0].plot(ma_episodes, mc_ma, color='blue', linewidth=2.5, label='Monte Carlo')\n",
    "        axes[0, 0].plot(ma_episodes, ql_ma, color='red', linewidth=2.5, label='Q-Learning')\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Episode', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Average Reward', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_title('Rewards Comparison (100-Episode MA)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend(fontsize=11)\n",
    "    axes[0, 0].grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # 2. Success rate comparison\n",
    "    axes[0, 1].plot(episodes, np.array(results_mc['success_rate']) * 100, \n",
    "                   color='blue', linewidth=2.5, label='Monte Carlo', alpha=0.8)\n",
    "    axes[0, 1].plot(episodes, np.array(results_ql['success_rate']) * 100, \n",
    "                   color='red', linewidth=2.5, label='Q-Learning', alpha=0.8)\n",
    "    axes[0, 1].fill_between(episodes, 0, np.array(results_mc['success_rate']) * 100, \n",
    "                           alpha=0.2, color='blue')\n",
    "    axes[0, 1].fill_between(episodes, 0, np.array(results_ql['success_rate']) * 100, \n",
    "                           alpha=0.2, color='red')\n",
    "    axes[0, 1].set_xlabel('Episode', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Success Rate (%)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_title('Success Rate Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].legend(fontsize=11)\n",
    "    axes[0, 1].grid(True, alpha=0.3, linestyle='--')\n",
    "    axes[0, 1].set_ylim(0, 105)\n",
    "    \n",
    "    # 3. Steps comparison\n",
    "    if len(results_mc['steps']) >= window:\n",
    "        mc_steps_ma = np.convolve(results_mc['steps'], np.ones(window)/window, mode='valid')\n",
    "        ql_steps_ma = np.convolve(results_ql['steps'], np.ones(window)/window, mode='valid')\n",
    "        \n",
    "        axes[1, 0].plot(ma_episodes, mc_steps_ma, color='blue', linewidth=2.5, label='Monte Carlo')\n",
    "        axes[1, 0].plot(ma_episodes, ql_steps_ma, color='red', linewidth=2.5, label='Q-Learning')\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Episode', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Average Steps', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_title('Steps to Completion (100-Episode MA)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend(fontsize=11)\n",
    "    axes[1, 0].grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # 4. Final performance statistics\n",
    "    mc_final_reward = np.mean(results_mc['rewards'][-100:])\n",
    "    ql_final_reward = np.mean(results_ql['rewards'][-100:])\n",
    "    mc_final_success = results_mc['success_rate'][-1] * 100\n",
    "    ql_final_success = results_ql['success_rate'][-1] * 100\n",
    "    mc_final_steps = np.mean(results_mc['steps'][-100:])\n",
    "    ql_final_steps = np.mean(results_ql['steps'][-100:])\n",
    "    \n",
    "    categories = ['Avg Reward\\n(last 100)', 'Success Rate\\n(%)', 'Avg Steps\\n(last 100)']\n",
    "    mc_values = [mc_final_reward, mc_final_success, mc_final_steps]\n",
    "    ql_values = [ql_final_reward, ql_final_success, ql_final_steps]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    max_vals = [max(mc_final_reward, ql_final_reward), \n",
    "                max(mc_final_success, ql_final_success), \n",
    "                max(mc_final_steps, ql_final_steps)]\n",
    "    \n",
    "    bars1 = axes[1, 1].bar(x - width/2, mc_values, width, label='Monte Carlo', \n",
    "                          color='blue', alpha=0.7)\n",
    "    bars2 = axes[1, 1].bar(x + width/2, ql_values, width, label='Q-Learning', \n",
    "                          color='red', alpha=0.7)\n",
    "    \n",
    "    axes[1, 1].set_ylabel('Value', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_title('Final Performance Metrics', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels(categories, fontsize=10)\n",
    "    axes[1, 1].legend(fontsize=11)\n",
    "    axes[1, 1].grid(True, alpha=0.3, linestyle='--', axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                          f'{height:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filename = f'output/plots/algorithm_comparison_{grid_size}.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved: {filename}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä ALGORITHM COMPARISON SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n{'Metric':<30} {'Monte Carlo':>15} {'Q-Learning':>15} {'Winner':>10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    winner_reward = \"MC\" if mc_final_reward > ql_final_reward else \"QL\"\n",
    "    winner_success = \"MC\" if mc_final_success > ql_final_success else \"QL\"\n",
    "    winner_steps = \"MC\" if mc_final_steps < ql_final_steps else \"QL\"  # Lower is better\n",
    "    \n",
    "    print(f\"{'Avg Reward (last 100)':<30} {mc_final_reward:>15.2f} {ql_final_reward:>15.2f} {winner_reward:>10}\")\n",
    "    print(f\"{'Success Rate (%)':<30} {mc_final_success:>15.1f} {ql_final_success:>15.1f} {winner_success:>10}\")\n",
    "    print(f\"{'Avg Steps (last 100)':<30} {mc_final_steps:>15.1f} {ql_final_steps:>15.1f} {winner_steps:>10}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Compare the two algorithms\n",
    "compare_algorithms(results_mc, results_ql, grid_size=f'{GRID_HEIGHT}x{GRID_WIDTH}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4700f5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üß™ TESTING MONTECARLO AGENT\n",
      "======================================================================\n",
      "Test  1/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test  2/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test  3/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test  4/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test  5/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test  6/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test  7/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test  8/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test  9/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 10/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 11/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 12/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 13/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 14/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 15/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 16/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 17/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 18/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 19/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 20/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Success Rate: 0/20 (0.0%)\n",
      "Average Steps: 50.0\n",
      "Average Reward: -50.0\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def test_agent(env, Q, num_tests=10, max_steps=50, algorithm='MonteCarlo'):\n",
    "    \"\"\"Test trained agent on new random environments.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"üß™ TESTING {algorithm.upper()} AGENT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    test_results = []\n",
    "    \n",
    "    for test_ep in range(1, num_tests + 1):\n",
    "        # Generate new random environment\n",
    "        reset(env, randomize=True)\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done and steps < max_steps:\n",
    "            s = tuple(env[\"agent_pos\"])\n",
    "            \n",
    "            # Greedy policy (no exploration)\n",
    "            action = np.argmax(Q[s[0], s[1]])\n",
    "            new_state, reward, done = step(env, action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        success = done and total_reward > 0\n",
    "        test_results.append({\n",
    "            'episode': test_ep,\n",
    "            'steps': steps,\n",
    "            'reward': total_reward,\n",
    "            'success': success\n",
    "        })\n",
    "        \n",
    "        status = \"‚úÖ SUCCESS\" if success else \"‚ùå FAILED\"\n",
    "        print(f\"Test {test_ep:2d}/{num_tests}: {status} | Steps: {steps:3d} | Reward: {total_reward:6.1f}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    successes = sum([r['success'] for r in test_results])\n",
    "    avg_steps = np.mean([r['steps'] for r in test_results])\n",
    "    avg_reward = np.mean([r['reward'] for r in test_results])\n",
    "    \n",
    "    print(f\"Success Rate: {successes}/{num_tests} ({successes/num_tests*100:.1f}%)\")\n",
    "    print(f\"Average Steps: {avg_steps:.1f}\")\n",
    "    print(f\"Average Reward: {avg_reward:.1f}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "# Test Monte Carlo agent\n",
    "test_results_mc = test_agent(env_mc, Q_mc, num_tests=20, algorithm='MonteCarlo')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24bd722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üß™ TESTING QLEARNING AGENT\n",
      "======================================================================\n",
      "Test  1/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test  2/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test  3/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test  4/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test  5/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test  6/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test  7/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test  8/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test  9/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 10/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 11/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 12/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 13/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 14/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 15/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 16/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 17/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 18/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 19/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "Test 20/20: ‚ùå FAILED | Steps:  50 | Reward:  -50.0\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Success Rate: 0/20 (0.0%)\n",
      "Average Steps: 50.0\n",
      "Average Reward: -50.0\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Q-Learning agent\n",
    "test_results_ql = test_agent(env_ql, Q_ql, num_tests=20, algorithm='QLearning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cae8598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üéÆ GRIDWORLD REINFORCEMENT LEARNING - TRAINING SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "üìê Grid Configuration: 10 √ó 10\n",
      "üîÑ Dynamic Environment: Goals and obstacles randomized each episode\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üé≤ MONTE CARLO RESULTS\n",
      "================================================================================\n",
      "Training Episodes: 10000\n",
      "Final Average Reward (last 100): -193.59\n",
      "Final Success Rate: 0.4%\n",
      "Final Average Steps: 192.4\n",
      "Test Success Rate (20 episodes): 0.0%\n",
      "\n",
      "================================================================================\n",
      "üîÑ Q-LEARNING RESULTS\n",
      "================================================================================\n",
      "Training Episodes: 10000\n",
      "Final Average Reward (last 100): -180.02\n",
      "Final Success Rate: 9.0%\n",
      "Final Average Steps: 181.0\n",
      "Test Success Rate (20 episodes): 0.0%\n",
      "\n",
      "================================================================================\n",
      "‚öîÔ∏è ALGORITHM COMPARISON\n",
      "================================================================================\n",
      "üèÜ Reward Winner: Q-Learning (-180.02 vs -193.59)\n",
      "üèÜ Success Winner: Q-Learning (9.0% vs 0.4%)\n",
      "üèÜ Efficiency Winner: Q-Learning (181.0 vs 192.4 steps)\n",
      "\n",
      "================================================================================\n",
      "üìÅ GENERATED OUTPUT FILES\n",
      "================================================================================\n",
      "\n",
      "üé¨ GIF Animations:\n",
      "  - Monte Carlo: episodes 1, 100, 500, 1000, 5000, 10000\n",
      "  - Q-Learning: episodes 1, 100, 500, 1000, 5000, 10000\n",
      "\n",
      "üìä Performance Plots:\n",
      "  - MonteCarlo_training_results.png\n",
      "  - MonteCarlo_convergence_analysis.png\n",
      "  - MonteCarlo_learned_policy.png\n",
      "  - QLearning_training_results.png\n",
      "  - QLearning_convergence_analysis.png\n",
      "  - QLearning_learned_policy.png\n",
      "  - algorithm_comparison.png\n",
      "\n",
      "üíæ Saved Models:\n",
      "  - monte_carlo_Q_table.npy\n",
      "  - q_learning_Q_table.npy\n",
      "\n",
      "================================================================================\n",
      "‚úÖ TRAINING COMPLETED SUCCESSFULLY\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Summary report saved: output/training_summary_report.txt\n"
     ]
    }
   ],
   "source": [
    "def generate_summary_report(results_mc, results_ql, test_results_mc, test_results_ql, \n",
    "                           grid_height, grid_width):\n",
    "    \"\"\"Generate and save a comprehensive summary report.\"\"\"\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"üéÆ GRIDWORLD REINFORCEMENT LEARNING - TRAINING SUMMARY REPORT\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(f\"\\nüìê Grid Configuration: {grid_height} √ó {grid_width}\")\n",
    "    report.append(f\"üîÑ Dynamic Environment: Goals and obstacles randomized each episode\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Monte Carlo Results\n",
    "    report.append(\"\\n\" + \"=\" * 80)\n",
    "    report.append(\"üé≤ MONTE CARLO RESULTS\")\n",
    "    report.append(\"=\" * 80)\n",
    "    mc_final_reward = np.mean(results_mc['rewards'][-100:])\n",
    "    mc_final_success = results_mc['success_rate'][-1] * 100\n",
    "    mc_final_steps = np.mean(results_mc['steps'][-100:])\n",
    "    mc_test_success = sum([r['success'] for r in test_results_mc]) / len(test_results_mc) * 100\n",
    "    \n",
    "    report.append(f\"Training Episodes: {len(results_mc['rewards'])}\")\n",
    "    report.append(f\"Final Average Reward (last 100): {mc_final_reward:.2f}\")\n",
    "    report.append(f\"Final Success Rate: {mc_final_success:.1f}%\")\n",
    "    report.append(f\"Final Average Steps: {mc_final_steps:.1f}\")\n",
    "    report.append(f\"Test Success Rate (20 episodes): {mc_test_success:.1f}%\")\n",
    "    \n",
    "    # Q-Learning Results\n",
    "    report.append(\"\\n\" + \"=\" * 80)\n",
    "    report.append(\"üîÑ Q-LEARNING RESULTS\")\n",
    "    report.append(\"=\" * 80)\n",
    "    ql_final_reward = np.mean(results_ql['rewards'][-100:])\n",
    "    ql_final_success = results_ql['success_rate'][-1] * 100\n",
    "    ql_final_steps = np.mean(results_ql['steps'][-100:])\n",
    "    ql_test_success = sum([r['success'] for r in test_results_ql]) / len(test_results_ql) * 100\n",
    "    \n",
    "    report.append(f\"Training Episodes: {len(results_ql['rewards'])}\")\n",
    "    report.append(f\"Final Average Reward (last 100): {ql_final_reward:.2f}\")\n",
    "    report.append(f\"Final Success Rate: {ql_final_success:.1f}%\")\n",
    "    report.append(f\"Final Average Steps: {ql_final_steps:.1f}\")\n",
    "    report.append(f\"Test Success Rate (20 episodes): {ql_test_success:.1f}%\")\n",
    "    \n",
    "    # Comparison\n",
    "    report.append(\"\\n\" + \"=\" * 80)\n",
    "    report.append(\"‚öîÔ∏è ALGORITHM COMPARISON\")\n",
    "    report.append(\"=\" * 80)\n",
    "    \n",
    "    if mc_final_reward > ql_final_reward:\n",
    "        report.append(f\"üèÜ Reward Winner: Monte Carlo ({mc_final_reward:.2f} vs {ql_final_reward:.2f})\")\n",
    "    else:\n",
    "        report.append(f\"üèÜ Reward Winner: Q-Learning ({ql_final_reward:.2f} vs {mc_final_reward:.2f})\")\n",
    "    \n",
    "    if mc_final_success > ql_final_success:\n",
    "        report.append(f\"üèÜ Success Winner: Monte Carlo ({mc_final_success:.1f}% vs {ql_final_success:.1f}%)\")\n",
    "    else:\n",
    "        report.append(f\"üèÜ Success Winner: Q-Learning ({ql_final_success:.1f}% vs {mc_final_success:.1f}%)\")\n",
    "    \n",
    "    if mc_final_steps < ql_final_steps:\n",
    "        report.append(f\"üèÜ Efficiency Winner: Monte Carlo ({mc_final_steps:.1f} vs {ql_final_steps:.1f} steps)\")\n",
    "    else:\n",
    "        report.append(f\"üèÜ Efficiency Winner: Q-Learning ({ql_final_steps:.1f} vs {mc_final_steps:.1f} steps)\")\n",
    "    \n",
    "    # Output files\n",
    "    report.append(\"\\n\" + \"=\" * 80)\n",
    "    report.append(\"üìÅ GENERATED OUTPUT FILES\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"\\nüé¨ GIF Animations:\")\n",
    "    report.append(\"  - Monte Carlo: episodes 1, 100, 500, 1000, 5000, 10000\")\n",
    "    report.append(\"  - Q-Learning: episodes 1, 100, 500, 1000, 5000, 10000\")\n",
    "    report.append(\"\\nüìä Performance Plots:\")\n",
    "    report.append(\"  - MonteCarlo_training_results.png\")\n",
    "    report.append(\"  - MonteCarlo_convergence_analysis.png\")\n",
    "    report.append(\"  - MonteCarlo_learned_policy.png\")\n",
    "    report.append(\"  - QLearning_training_results.png\")\n",
    "    report.append(\"  - QLearning_convergence_analysis.png\")\n",
    "    report.append(\"  - QLearning_learned_policy.png\")\n",
    "    report.append(\"  - algorithm_comparison.png\")\n",
    "    report.append(\"\\nüíæ Saved Models:\")\n",
    "    report.append(\"  - monte_carlo_Q_table.npy\")\n",
    "    report.append(\"  - q_learning_Q_table.npy\")\n",
    "    \n",
    "    report.append(\"\\n\" + \"=\" * 80)\n",
    "    report.append(\"‚úÖ TRAINING COMPLETED SUCCESSFULLY\")\n",
    "    report.append(\"=\" * 80)\n",
    "    \n",
    "    # Print report\n",
    "    report_text = \"\\n\".join(report)\n",
    "    print(report_text)\n",
    "    \n",
    "    # Save report to file\n",
    "    with open('output/training_summary_report.txt', 'w') as f:\n",
    "        f.write(report_text)\n",
    "    \n",
    "    print(\"\\n‚úÖ Summary report saved: output/training_summary_report.txt\")\n",
    "\n",
    "# Generate the summary report\n",
    "generate_summary_report(results_mc, results_ql, test_results_mc, test_results_ql, \n",
    "                       GRID_HEIGHT, GRID_WIDTH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
